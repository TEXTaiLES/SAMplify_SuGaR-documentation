{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"NEPHELE DOCUMENTATION PORTAL","text":"<p> NEPHELE is an intermediate reconstruction module developed within the  TEXTaiLES toolbox as a dedicated  3D reconstruction component. It serves as the central bridge between SAM2 segmentation  and SuGaR-based surface alignment. </p> <p> NEPHELE transforms segmented multi-view images into a structured 3D Gaussian  representation. Named after the mythological figure Nephele, symbolizing  cloud and formation, the module embodies the core principle of Gaussian  Splatting: representing geometry as a continuous density cloud rather than  discrete points. </p> <p> Using SAM2-filtered masks and COLMAP camera parameters, NEPHELE constructs an  initial Gaussian field where each splat encodes localized surface attributes such  as position, covariance, opacity, and anisotropy. This cloud-based representation  provides a smooth and differentiable foundation for SuGaR optimization, enhancing  reconstruction stability, improving surface consistency across views, and enabling  efficient convergence toward high-quality 3D models. </p> <p> </p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this software, please cite it using the following BibTeX entry:</p> <pre><code>@software{Nephele_TEXTaiLES_2026,\n  author  = {{Athena Research Center}},\n  title   = {{Nephele: SAM2 + COLMAP + SuGaR pipeline for background-free 3D mesh reconstruction}},\n  url     = {https://github.com/TEXTaiLES/SAMplify_SuGaR},\n  version = {0.1.0},\n  year    = {2025},\n  license = {MIT}\n}\n</code></pre>"},{"location":"#third-party-citations","title":"Third-party citations","text":"<pre><code>@article{ravi2024sam2,\n  title={SAM 2: Segment Anything in Images and Videos},\n  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\\\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\\'a}r, Piotr and Feichtenhofer, Christoph},\n  journal={arXiv preprint arXiv:2408.00714},\n  url={https://arxiv.org/abs/2408.00714},\n  year={2024}\n}\n\n@inproceedings{Schonberger2016SfM,\n  title     = {Structure-from-Motion Revisited},\n  author    = {Sch{\\\"o}nberger, Johannes L. and Frahm, Jan-Michael},\n  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year      = {2016}\n}\n\n@article{guedon2023sugar,\n  title   = {SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering},\n  author  = {Gu{\\'e}don, Antoine and Lepetit, Vincent},\n  journal = {CVPR},\n  year    = {2024}\n}\n\n@article{kerbl2023gaussiansplatting,\n  title   = {3D Gaussian Splatting for Real-Time Radiance Field Rendering},\n  author  = {Kerbl, Bernhard and Kopanas, Georgios and Leimk{\\\"u}hler, Thomas and Drettakis, George},\n  journal = {ACM Transactions on Graphics},\n  year    = {2023}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. </p>"},{"location":"deployment/dataset_preparation/","title":"Dataset Preparation","text":"<p>This page describes how to prepare your dataset for the SAMplify_SuGaR pipeline. A properly structured dataset ensures that SAM2, COLMAP, and SuGaR process the images correctly.</p>"},{"location":"deployment/dataset_preparation/#1-dataset-location","title":"1. Dataset Location","text":"<p>All datasets must be placed inside the SAM2 directory:</p> <pre><code>\nSAM2/\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 input/\n        \u2514\u2500\u2500 &lt;DATASET_NAME&gt;/\n</code></pre> <p>The folder <code>&lt;DATASET_NAME&gt;</code> is chosen by you and will be used later when running the pipeline.</p> <p>Example:</p> <pre><code>\nSAM2/data/input/my_object/\n</code></pre>"},{"location":"deployment/dataset_preparation/#2-image-requirements","title":"2. Image Requirements","text":"<p>Inside your dataset folder, place your .jpg images:</p> <pre><code>\nSAM2/data/input/&lt;DATASET_NAME&gt;/\n\u251c\u2500\u2500 image1.jpg\n\u251c\u2500\u2500 image2.jpg\n\u251c\u2500\u2500 image3.jpg\n\u2514\u2500\u2500 ...\n\n</code></pre>"},{"location":"deployment/dataset_preparation/#image-guidelines","title":"Image guidelines:","text":"<ul> <li>Format must be .jpg</li> <li>Images should show the object from multiple viewpoints</li> <li>Avoid motion blur or overexposed frames</li> <li>Keep the object centered when possible</li> <li>Neutral background improves mask accuracy but is not required</li> </ul>"},{"location":"deployment/dataset_preparation/#3-minimum-dataset-size","title":"3. Minimum Dataset Size","text":"<p>For stable reconstruction:</p> <ul> <li>100-500 images recommended  </li> <li>More views = better Gaussian and mesh quality </li> <li>Avoid duplicate or identical frames  </li> </ul>"},{"location":"deployment/dataset_preparation/#4-mask-output-directory","title":"4. Mask Output Directory","text":"<p>After masking with SAM2, the pipeline automatically generates:</p> <pre><code>\nSAM2/data/output/&lt;DATASET_NAME&gt;/\n\u251c\u2500\u2500 masks/\n\u2514\u2500\u2500 masked_images/\n\n</code></pre> <p>You do not need to create this manually \u2014 the pipeline creates it.</p>"},{"location":"deployment/dataset_preparation/#5-verification","title":"5. Verification","text":"<p>Before running the pipeline, make sure:</p> <p>\u2714 A folder exists at: </p> <pre><code>`SAM2/data/input/&lt;DATASET_NAME&gt;/`  \n\n</code></pre> <p>\u2714 It contains at least several <code>.jpg</code> images  </p> <p>\u2714 File names do not contain spaces (recommended)  </p>"},{"location":"deployment/dataset_preparation/#6-next-step","title":"6. Next Step","text":"<p>After preparing your dataset, continue to:</p> <p>Running the Full Pipeline</p> <pre><code>(`run_pipeline.md`)\n\n</code></pre>"},{"location":"deployment/installation_basic/","title":"Basic Installation","text":"<p>This page describes how to install the SAMplify_SuGaR pipeline along with all required submodules and dependencies.</p>"},{"location":"deployment/installation_basic/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>Begin by cloning the full repository along with its submodules:</p> <pre><code>git clone --recurse-submodules https://github.com/ZachariVaia/SAMplify_SuGaR.git\ncd SAMplify_SuGaR\n</code></pre> <p>If you forgot to include <code>--recurse-submodules</code>, run:</p> <pre><code>git submodule update --init --recursive\n</code></pre> <p>This will download:</p> <ul> <li>SAM2 (mask generation module)</li> <li>SuGaR (Gaussian Splatting module)</li> <li>Additional scripts required for the pipeline</li> </ul>"},{"location":"deployment/run_pipeline/","title":"Running the Full Pipeline","text":"<p>This page explains how to execute the complete SAMplify_SuGaR pipeline, which runs:</p> <ol> <li>SAM2 \u2013 mask annotation and generation  </li> <li>Mask application  </li> <li>COLMAP \u2013 sparse &amp; dense reconstruction  </li> <li>SuGaR \u2013 Gaussian optimization  </li> <li>Mesh extraction  </li> </ol> <p>Once your dataset is correctly placed inside <code>SAM2/data/input/&lt;DATASET_NAME&gt;/</code>, you can run the pipeline with a single command.</p>"},{"location":"deployment/run_pipeline/#1-pipeline-command","title":"1. Pipeline Command","text":"<p>From the project root directory:</p> <pre><code>./run_pipeline.sh &lt;dataset_name&gt;\n</code></pre> <p>Example:</p> <pre><code>./run_pipeline.sh my_object\n</code></pre> <p>The <code>&lt;dataset_name&gt;</code> must match the folder name you created under:</p> <pre><code>SAM2/data/input/&lt;dataset_name&gt;/\n</code></pre>"},{"location":"deployment/run_pipeline/#2-what-the-pipeline-does","title":"2. What the Pipeline Does","text":"<p>The script performs the following steps automatically:</p>"},{"location":"deployment/run_pipeline/#step-1-sam2-mask-generation","title":"Step 1 \u2014 SAM2 Mask Generation","text":"<ul> <li>Launches SAM2</li> <li>Processes each image</li> <li>Saves binary masks into:</li> </ul> <pre><code>SAM2/data/output/&lt;dataset_name&gt;/masks/\n</code></pre>"},{"location":"deployment/run_pipeline/#step-2-mask-application","title":"Step 2 \u2014 Mask Application","text":"<p>The pipeline multiplies each mask with the original image:</p> <pre><code>masked = original_image * mask\n</code></pre> <p>Masked images are saved into:</p> <pre><code>SAM2/data/output/&lt;dataset_name&gt;/masked_images/\n</code></pre> <p>These images are used as input to COLMAP.</p>"},{"location":"deployment/run_pipeline/#step-3-colmap-reconstruction","title":"Step 3 \u2014 COLMAP Reconstruction","text":"<p>COLMAP runs:</p> <ul> <li>Sparse reconstruction</li> <li>Pose estimation</li> <li>Bundle adjustment</li> <li>Dense stereo</li> <li>Depth fusion</li> </ul> <p>Results are saved under:</p> <pre><code>output/&lt;dataset_name&gt;/colmap/\n \u251c\u2500\u2500 sparse/\n \u2514\u2500\u2500 dense/\n</code></pre>"},{"location":"deployment/run_pipeline/#step-4-sugar-optimization-gaussian-splatting","title":"Step 4 \u2014 SuGaR Optimization (Gaussian Splatting)","text":"<p>The dense cloud from COLMAP initializes the Gaussian field. SuGaR then optimizes:</p> <ul> <li>Gaussian positions</li> <li>Rotations</li> <li>Scales</li> <li>Covariance</li> <li>Opacity</li> </ul> <p>Outputs appear in:</p> <pre><code>output/&lt;dataset_name&gt;/sugar/\n</code></pre>"},{"location":"deployment/run_pipeline/#step-5-mesh-extraction","title":"Step 5 \u2014 Mesh Extraction","text":"<p>A clean, background-free mesh is generated using:</p> <ul> <li>SDF estimation</li> <li>Poisson reconstruction or marching cubes</li> <li>Texture projection (optional)</li> </ul> <p>Final meshes are saved in:</p> <pre><code>output/&lt;dataset_name&gt;/final/\n</code></pre>"},{"location":"deployment/run_pipeline/#3-logs-and-console-output","title":"3. Logs and Console Output","text":"<p>During execution you will see:</p> <ul> <li>Masking progress</li> <li>COLMAP reconstruction logs</li> <li>SuGaR training progress</li> <li>Mesh extraction messages</li> </ul> <p>If any component fails, check:</p> <pre><code>output/&lt;dataset_name&gt;/logs/\n</code></pre>"},{"location":"deployment/run_pipeline/#4-pipeline-completion","title":"4. Pipeline Completion","text":"<p>When the pipeline finishes successfully, you will have:</p> <ul> <li>All masks</li> <li>Masked images</li> <li>COLMAP reconstruction</li> <li>SuGaR Gaussian field</li> <li>Final mesh model</li> </ul> <p>Stored neatly in the output folder.</p>"},{"location":"deployment/run_pipeline/#5-next-steps","title":"5. Next Steps","text":"<p>Proceed to the Manual section to understand each module:</p> <ul> <li>SAM2 GUI</li> <li>COLMAP processing</li> <li>SuGaR Gaussian optimization</li> <li>Mesh extraction</li> </ul>"},{"location":"manual/colmap/","title":"COLMAP Introduction","text":"<p>COLMAP is an open-source photogrammetry software used to reconstruct 3D point clouds from images. In this project, COLMAP is utilized to generate the point cloud data that serves as the foundation for the SuGaR pipeline.</p> <p>For detailed instructions and advanced usage, please refer to the official COLMAP documentation: COLMAP Documentation.</p>"},{"location":"manual/sam2_gui/","title":"SAM2 Manual \u2014 Interactive Point Picker","text":"<p>The SAM2 module handles the segmentation step of the pipeline. When you run the full pipeline using:</p> <pre><code>\n./run_pipeline.sh &lt;dataset_name&gt;\n\n</code></pre> <p>SAM2 automatically launches a browser-based interface on:</p> <pre><code>\n[http://localhost:8092](http://localhost:8092)\n\n</code></pre> <p>This interface allows you to annotate the dataset using interactive point picking. It is the only step where the user provides manual input.</p>"},{"location":"manual/sam2_gui/#1-point-picker-interface-overview","title":"1. Point Picker Interface Overview","text":"<p>When the interface opens, you will see a screen like the one below:</p> <p></p> <ul> <li>Inspect each frame of your dataset  </li> <li>Add foreground points on the object  </li> <li>Add background points on unwanted regions  </li> <li>Save the prompts (<code>prompts.json</code>) used for mask generation  </li> </ul> <p>SAM2 supports two modes, but the pipeline uses:</p> <p>** Interactive Picker (recommended)**</p>"},{"location":"manual/sam2_gui/#2-first-time-use","title":"2. First-Time Use","text":"<p>When processing a dataset for the first time, the following files do not exist yet:</p> <pre><code>\nSAM2/data/output/&lt;dataset_name&gt;/prompts.json\nSAM2/data/output/&lt;dataset_name&gt;/__picker_done.flag\n\n</code></pre> <p>So SAM2 will only show:</p>"},{"location":"manual/sam2_gui/#create-new","title":"Create new","text":"<p>This means you must annotate the dataset manually:</p> <ol> <li>Click inside the image to add foreground points  </li> <li>Add background points to remove clutter  </li> <li>Repeat for all images  </li> <li>Click \u201cSave\u201d at the end  </li> </ol> <p>SAM2 will automatically generate:</p> <pre><code>\nSAM2/data/output/&lt;dataset_name&gt;/prompts.json\nSAM2/data/output/&lt;dataset_name&gt;/__picker_done.flag\n\n</code></pre> <p>These files store your segmentation instructions.</p>"},{"location":"manual/sam2_gui/#21-annotating-new-points-foreground-background","title":"2.1 Annotating New Points (Foreground / Background)","text":"<p>If you choose Create new, SAM2 will guide you through the full manual annotation process. You will first see the annotation interface:</p> <p></p> <p>Here you can click on the image and define which areas belong to the object (foreground) and which belong to the background.</p>"},{"location":"manual/sam2_gui/#foreground-vs-background-points","title":"\ud83d\udfe2 Foreground vs \ud83d\udd34 Background Points","text":"<p>SAM2 uses two types of points:</p> Click Color Meaning Left click \ud83d\udfe2 Green Foreground (object to keep) Right click \ud83d\udd34 Red Background (remove this area) <p>Example:</p> <p></p> <ul> <li>Use right-click to add green points on the object  </li> <li>Use right-click to add red points on background or areas to exclude  </li> <li>Use the mouse wheel to zoom  </li> <li>Hold Space + Drag to pan  </li> <li>Press U or click Undo to remove the last point  </li> </ul> <p>Once you've placed your points, click Save. SAM2 will generate a mask preview.</p>"},{"location":"manual/sam2_gui/#preview-generation","title":"Preview Generation","text":"<p>After clicking Save, the system shows:</p> <p></p> <p>This stage computes the segmentation based on the points you provided.</p>"},{"location":"manual/sam2_gui/#reviewing-several-masked-frames","title":"Reviewing Several Masked Frames","text":"<p>SAM2 then displays a gallery of sample frames with masks applied:</p> <p></p> <p>Here you should check:</p> <ul> <li>If the mask correctly follows the object  </li> <li>If important details are preserved  </li> <li>If background is eliminated properly  </li> <li>If the mask quality is acceptable across multiple frames  </li> </ul> <p>If the result is not good, click Start over and refine your points.</p> <p>If the result looks correct, click Looks good, continue.</p>"},{"location":"manual/sam2_gui/#final-confirmation","title":"Final Confirmation","text":"<p>Once the masks are accepted, SAM2 displays a confirmation message:</p> <p></p> <p>At this point:</p> <ul> <li>Your prompts have been saved  </li> <li>The <code>prompts.json</code> and <code>__picker_done.flag</code> files are created  </li> <li>The pipeline continues automatically (COLMAP starts next)  </li> </ul> <p>You can safely close the browser tab and return to the terminal.</p>"},{"location":"manual/sam2_gui/#3-using-existing-prompts-second-run","title":"3. Using Existing Prompts (Second Run)","text":"<p>If a previous annotation exists, the picker will detect these files:</p> <pre><code>\nprompts.json\n__picker_done.flag\n\n</code></pre> <p>Then the interface will show two buttons:</p>"},{"location":"manual/sam2_gui/#use-existing","title":"Use existing","text":"<p>Loads your saved points automatically.</p>"},{"location":"manual/sam2_gui/#create-new_1","title":"Create new","text":"<p>Starts annotation from scratch and overwrites the old prompts.</p> <p>This allows you to redo segmentation only when needed, instead of repeating the entire process.</p>"},{"location":"manual/sam2_gui/#4-files-created-by-sam2","title":"4. Files Created by SAM2","text":"<p>After annotation, SAM2 generates the following directory structure:</p> <pre><code>\nSAM2/data/output/&lt;dataset_name&gt;/\n\u251c\u2500\u2500 prompts.json\n\u251c\u2500\u2500 __picker_done.flag\n\u251c\u2500\u2500 __use_existing.flag          (created only if \"Use existing\" is selected)\n\u251c\u2500\u2500 masks/\n\u2514\u2500\u2500 masked_images/\n\n</code></pre>"},{"location":"manual/sam2_gui/#file-descriptions","title":"File Descriptions","text":"File Purpose prompts.json Stores all user clicks for segmentation __picker_done.flag Indicates that annotation is complete __use_existing.flag Indicates that old prompts should be reused masks/ Binary masks generated from SAM2 masked_images/ Images multiplied by the mask, used by COLMAP"},{"location":"manual/sam2_gui/#5-what-happens-after-point-picking","title":"5. What Happens After Point Picking","text":"<p>Once the annotation step is done:</p> <ol> <li>SAM2 generates masks  </li> <li>SAM2 generates masked images  </li> <li>The pipeline automatically continues to COLMAP  </li> </ol> <p>You do not need to run anything manually.</p>"},{"location":"manual/sam2_gui/#6-tips-for-best-mask-quality","title":"6. Tips for Best Mask Quality","text":"<ul> <li>Add 3\u20137 foreground points on the object  </li> <li>Add background points for shadows, reflections, or clutter  </li> <li>Zoom into edges to improve accuracy  </li> <li>Check frames with different lighting or unusual angles  </li> <li>Make sure the object is not clipped by the mask  </li> </ul> <p>Even a few accurate prompts dramatically improve reconstruction quality.</p>"},{"location":"manual/sam2_gui/#7-troubleshooting","title":"7. Troubleshooting","text":""},{"location":"manual/sam2_gui/#the-gui-does-not-open","title":"\u2757 The GUI does not open","text":"<p>Try manually visiting:</p> <pre><code>\n[http://127.0.0.1:8092](http://127.0.0.1:8092)\n\n</code></pre> <p>If it still fails:</p> <ul> <li>Check that no other app uses port 8092  </li> <li>Restart the pipeline  </li> <li>Clear browser cache  </li> </ul>"},{"location":"manual/sam2_gui/#masks-look-incorrect","title":"\u2757 Masks look incorrect","text":"<ul> <li>Select Create new and re-annotate  </li> <li>Add more foreground/background points  </li> <li>Focus on difficult frames (shiny, dark, motion blur)  </li> </ul>"},{"location":"manual/sam2_gui/#8-summary","title":"8. Summary","text":"<p>The SAM2 Interactive Picker is the only manual step of the pipeline. Once annotation is complete, the rest of the reconstruction process is fully automatic.</p> <p>You are now ready to proceed to:</p> <p>COLMAP Manual SuGaR Optimization Manual</p>"},{"location":"manual/sugar/","title":"SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering","text":"<p>SuGaR (Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering) is a method used for the segmentation, annotation, and reconstruction of 3D point clouds and meshes. In this project, SuGaR is employed to process the point cloud generated by COLMAP, enabling efficient and accurate surface annotation and mesh reconstruction.</p> <p>For a detailed explanation of the method, code, and additional resources, please refer to:</p> <ul> <li>Official SuGaR repository (GitHub)</li> <li>SuGaR project webpage</li> </ul>"}]}